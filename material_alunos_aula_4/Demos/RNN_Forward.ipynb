{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Forward.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sk3TpLb8DhT",
        "colab_type": "text"
      },
      "source": [
        "# RNN Forward\n",
        "\n",
        "Imports básicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heM-v_e475r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic imports.\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xONfLuDyDvSB",
        "colab_type": "code",
        "outputId": "d01217ce-9865-4760-e8cc-bf42ec5bb094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNqxMX9v8YC-",
        "colab_type": "text"
      },
      "source": [
        "## Relembrando Redes Neurais Feed-Forward\n",
        "Um perceptron comum, que integra uma Rede Neural Feed-Forward, realiza a seguinte operaçao:\n",
        "\n",
        "\\begin{align*}\n",
        "f(X) = \\sigma (W_{xh}X + b_h),\n",
        "\\end{align*}\n",
        "\n",
        "sendo \n",
        "- $X \\in R^{b \\times d}$ um mini batch de $b$ amostras com $d$ dimensões;\n",
        "- $W_{xh} \\in R^{d \\times h}$ a matriz de pesos, com $h$ sendo a dimensionalidade da saída desejada;\n",
        "- $b_h \\in R^{1 \\times h}$ o viés;\n",
        "- $\\sigma$ a ativação não linear.\n",
        "\n",
        "O resultado é $f(X) \\in R^{b \\times h}$, podendo tanto ser a feature intermediária que alimentará camadas futuras, ou a saída final do modelo. \n",
        "\n",
        "O código a seguir implementa um forward do zero de uma camada feed forward simples.  A nomenclatura *hidden_size* representa a dimensionalidade da saída da camada.\n",
        "\n",
        "Hiperparâmetros\n",
        "- batch size = 10\n",
        "- input size = 100\n",
        "- hidden size = 512\n",
        "- função de ativação = tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVvQhuKw8ZLI",
        "colab_type": "code",
        "outputId": "f97971b5-25e4-4025-a642-421f217752ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def init_weights(input_size, hidden_size):\n",
        "  params = {}\n",
        "  params['Wxh'] = np.random.randn(input_size, hidden_size)\n",
        "  params['bh']  = np.random.randn(1, hidden_size)\n",
        "  \n",
        "  return params\n",
        "\n",
        "def forward(X, params):\n",
        "  return np.tanh(np.dot(X, params['Wxh']) + params['bh'])\n",
        "\n",
        "batch_size  = 10\n",
        "input_size  = 100\n",
        "hidden_size = 512\n",
        "\n",
        "## Generate random batch \n",
        "X = np.random.randn(batch_size, input_size)\n",
        "\n",
        "## Init weights\n",
        "params = init_weights(input_size, hidden_size)\n",
        "\n",
        "## Forward\n",
        "output = forward(X, params)\n",
        "print(output.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXfFTIPN94iD",
        "colab_type": "text"
      },
      "source": [
        "## Redes Recorrentes - *Forward from Scratch*\n",
        "\n",
        "Uma rede neural recorrente, por outro lado, é uma função não só de $X$, mas também de um *hidden state* $H \\in R^{b \\times h}$, que representa a memória interna da unidade recorrente. Em outras palavras, a unidade recorrente é uma função da entrada atual $X_t$ e do conhecimento prévio acumulado $H_{t-1}$, com $t$ representando os timesteps. Para tal, adiciona-se uma matriz de pesos $W_{hh} \\in R^{h \\times h}$ que opera com a memória interna de timesteps anteriores, ou seja\n",
        "\n",
        "\\begin{align*}\n",
        "f(X_t, H_{t-1}) = \\sigma (W_{xh}X_t + W_{hh}H_{t-1} + b_h).\n",
        "\\end{align*}\n",
        "\n",
        "Esse é um processo **iterativo**, que deve ser executado com todos os elementos $X_t$ da sequência de entrada $X = \\{X_1, X_2,..., X_n\\}$, sendo $n$ o tamanho da sequência. O forward produz o conjunto de *hidden states* $H = \\{H_1, H_2, ..., H_n\\}$.\n",
        "\n",
        "**Atenção:** Na primeira iteração, o *hidden state* $H_0$ deve ser inicializado. Existem vários métodos de inicialização, sendo os mais comuns inicialização aleatória ou com zeros. Veja a seguir uma ilustração do processo iterativo na versão compacta (esquerda) e desenrolada (direita):\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13lVkpXH5GqE8YjgpGyo8fFzRBKqPder7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAQ5Yxg-gnN",
        "colab_type": "text"
      },
      "source": [
        "A célula a seguir contém o forward iterativo de uma camada recorrente simples. A saída apresenta é o $H \\in R^{n \\times b \\times h}$ contendo todos os hidden states produzidos ao longo das iterações. \n",
        "\n",
        "**Atenção:** A entrada de uma unidade recorrente **não** segue mais o padrão $(B,C,H,W)$ que estávamos acostumados, seguindo agora o padrão $(N,B,D)$ sendo $N$ o tamanho da sequência e $D$ a dimensionalidade de cada elemento da sequência.\n",
        "\n",
        "Hiperparâmetros\n",
        "- **sequence length** = 50\n",
        "- batch size = 10\n",
        "- input size = 100\n",
        "- hidden size = 512\n",
        "- função de ativação = tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyvMkrH096Gc",
        "colab_type": "code",
        "outputId": "e3bc9128-6803-487e-ef48-5934ce1eda22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def init_weights(input_size, hidden_size):\n",
        "  params = {}\n",
        "  params['Wxh'] = np.random.randn(input_size, hidden_size)\n",
        "  params['Whh'] = np.random.randn(hidden_size, hidden_size)\n",
        "  params['bh']  = np.random.randn(1, hidden_size)\n",
        "  \n",
        "  return params\n",
        "\n",
        "def forward(X, params):\n",
        "  \n",
        "  # Initialize hidden state for first iteration\n",
        "  H = np.random.randn(batch_size, hidden_size)\n",
        "  \n",
        "  # Iterative forward\n",
        "  hidden_states = []\n",
        "  for x in X:\n",
        "    H = np.tanh(np.dot(x, params['Wxh']) + np.dot(H, params['Whh']) + params['bh'] ) \n",
        "    hidden_states.append(H)\n",
        "    \n",
        "  return np.array(hidden_states) # output (seq_len, batch_size, hidden_size)\n",
        "\n",
        "seq_len     = 50\n",
        "batch_size  = 10\n",
        "input_size  = 100\n",
        "hidden_size = 512\n",
        "\n",
        "## Generate random batch \n",
        "X = np.random.randn(seq_len, batch_size, input_size)\n",
        "\n",
        "## Init weights\n",
        "params = init_weights(input_size, hidden_size)\n",
        "\n",
        "## Forward\n",
        "output = forward(X, params)\n",
        "print(output.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 10, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gKU7tcbfdc",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch RNNCell\n",
        "\n",
        "Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell\n",
        "\n",
        "Essa camada realiza a operação de uma célula básica de RNN, que já conhecemos pela definição a seguir\n",
        "\n",
        "\\begin{align*}\n",
        "H_t = f(X_t, H_{t-1}) = \\sigma (W_{xh}X_t + W_{hh}H_{t-1} + b_h).\n",
        "\\end{align*}\n",
        "\n",
        "O forward é realizado de forma idêntica ao que fizemos anteriormente, iterando na sequência para operar com cada um dos elementos. A saída a cada timestep $t$ é a memória atualizada $H_t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliwL4HuibAM",
        "colab_type": "code",
        "outputId": "8e25962c-dfb1-4fd1-dddf-65f0db5f81a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, batch_size):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size  = batch_size\n",
        "    \n",
        "    self.rnn = nn.RNNCell(input_size, hidden_size)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    # Initialize hidden state for first iteration\n",
        "    H = torch.randn(self.batch_size, self.hidden_size).to(args['device'])\n",
        "\n",
        "    # Iterative forward\n",
        "    hidden_states = []\n",
        "    for x in X:\n",
        "      H = self.rnn(x, H)\n",
        "      hidden_states.append(H)\n",
        "\n",
        "    return torch.stack(hidden_states) # output (seq_len, batch_size, hidden_size)\n",
        "\n",
        "net = RNN(input_size, hidden_size, batch_size).to(args['device'])\n",
        "  \n",
        "seq_len     = 50\n",
        "batch_size  = 10\n",
        "input_size  = 100\n",
        "hidden_size = 512\n",
        "\n",
        "## Generate random batch \n",
        "X = torch.randn(seq_len, batch_size, input_size).to(args['device'])\n",
        "\n",
        "## Forward\n",
        "output = net(X)\n",
        "print(output.size())\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 10, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}