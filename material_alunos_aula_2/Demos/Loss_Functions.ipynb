{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Loss_Functions (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8V3wN0gS-vKk"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r00eE5sW-vKn",
        "colab": {}
      },
      "source": [
        "from skimage import io\n",
        "from skimage import transform\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "!wget https://upload.wikimedia.org/wikipedia/en/thumb/7/7d/Lenna_%28test_image%29.png/220px-Lenna_%28test_image%29.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Tg6W4OX-vKt",
        "colab": {}
      },
      "source": [
        "# Customized Network.\n",
        "class CustomNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, num_classes=2):\n",
        "\n",
        "        super(CustomNetwork, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 4, 3, 1, 1),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2), # 256 -> 128\n",
        "\n",
        "            nn.Conv2d(4, 8, 3, 1, 1),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True), # 128 -> 64\n",
        "\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(8, 4, kernel_size=2, stride=2, padding=0, output_padding=0), # 64 -> 128\n",
        "\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(4, 1, kernel_size=2, stride=2, padding=0, output_padding=0), # 128 -> 256\n",
        "        )\n",
        "        \n",
        "        self.initialize_weights()\n",
        "    \n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        print('x: ', x.size())\n",
        "        \n",
        "        # Feature extraction.\n",
        "        x_hat = self.conv(x)\n",
        "        print('x_hat: ', x_hat.size())\n",
        "        \n",
        "        return x_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J4_VQFI9-vKx",
        "colab": {}
      },
      "source": [
        "def norm(img):\n",
        "    \n",
        "    mn = img.min()\n",
        "    mx = img.max()\n",
        "    \n",
        "    img = (img - mn) / (mx - mn)\n",
        "    \n",
        "    return img\n",
        "\n",
        "# Loading image, resizing to 256x256 and normalizing.\n",
        "img = io.imread('220px-Lenna_(test_image).png')\n",
        "io.imshow(img)\n",
        "img = transform.resize(img, (256, 256)).astype(np.float32)\n",
        "if len(img.shape) > 2:\n",
        "    img = img[:,:,0]\n",
        "\n",
        "img = norm(img)\n",
        "\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JziBd9x3-vK0",
        "colab": {}
      },
      "source": [
        "# Transforming to tensor.\n",
        "tensor = torch.from_numpy(img)\n",
        "tensor = tensor.unsqueeze(0)\n",
        "tensor = tensor.unsqueeze(0)\n",
        "# tensor = tensor.cuda() # For GPU casting.\n",
        "\n",
        "tensor = Variable(tensor, requires_grad=False)\n",
        "\n",
        "print(tensor.size())\n",
        "print(tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QzAxHJqO-vK3",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Instancing Network.\n",
        "net = CustomNetwork(1, 2)\n",
        "# net = CustomNetwork(1, 2).cuda() # For GPU casting.\n",
        "\n",
        "# Instancing Loss.\n",
        "#loss = nn.MSELoss()\n",
        "loss = nn.L1Loss()\n",
        "\n",
        "# Forwarding image.\n",
        "output = net(tensor)\n",
        "# output.requires_grad = False\n",
        "output = output.detach() # For removing differentiation requirement.\n",
        "\n",
        "# Computing loss.\n",
        "rec_loss = loss(tensor, output)\n",
        "\n",
        "print('reconstruction loss: ', rec_loss)\n",
        "\n",
        "# Plotting images\n",
        "fig, ax = plt.subplots(1, 2, figsize = (16, 8))\n",
        "\n",
        "ax[0].imshow(tensor.numpy()[0, 0])\n",
        "ax[0].set_title('Imagem Original')\n",
        "\n",
        "ax[1].imshow(output.numpy()[0, 0])\n",
        "ax[1].set_title('Imagem Reconstruída')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fXlZ1Lvr-vK7",
        "colab": {}
      },
      "source": [
        "def custom_loss(inp, tar):\n",
        "    return torch.mean(torch.abs(inp - tar))\n",
        "\n",
        "# Instancing Network.\n",
        "net = CustomNetwork(1, 2)\n",
        "# net = CustomNetwork(1, 2).cuda() # For GPU casting.\n",
        "\n",
        "# Forwarding image.\n",
        "output = net(tensor)\n",
        "# output.requires_grad = False\n",
        "output = output.detach() # For removing differentiation requirement.\n",
        "\n",
        "# Computing loss.\n",
        "rec_loss = custom_loss(tensor, output)\n",
        "\n",
        "print('reconstruction loss: ', rec_loss)\n",
        "\n",
        "# Plotting images\n",
        "fig, ax = plt.subplots(1, 2, figsize = (16, 8))\n",
        "\n",
        "ax[0].imshow(tensor.numpy()[0, 0])\n",
        "ax[0].set_title('Imagem Original')\n",
        "\n",
        "ax[1].imshow(output.numpy()[0, 0])\n",
        "ax[1].set_title('Imagem Reconstruída')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}