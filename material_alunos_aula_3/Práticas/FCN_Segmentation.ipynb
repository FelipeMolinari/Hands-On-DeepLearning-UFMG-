{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCN_Segmentation.ipynb","provenance":[{"file_id":"1WtiL2zGQAjFdx9Q2VbIo1EQmwrYKyTTm","timestamp":1581685738322}],"authorship_tag":"ABX9TyP/8snrcNFDlaZRFPv8fpq1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EQ2zfah7o9BZ","colab_type":"code","colab":{}},"source":["# Basic imports.\n","import os\n","import time\n","import numpy as np\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from torch.utils import data\n","from torch.backends import cudnn\n","\n","from torchvision import transforms\n","from torchvision import datasets\n","from torchvision import models\n","\n","from skimage import io\n","from skimage import transform\n","\n","from sklearn import metrics\n","\n","from matplotlib import pyplot as plt\n","\n","%matplotlib inline\n","\n","cudnn.benchmark = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pN_T4mXlpR9o","colab_type":"code","colab":{}},"source":["# Setting predefined arguments.\n","args = {\n","    'epoch_num': 50,      # Number of epochs.\n","    'n_classes': 3, #19,      # Number of classes in segmentation task.\n","    'pretrained': True,   # Boolean indicating pretraining of backbone.\n","    'skip': True,         # Boolean indicating presence of Skip Connections.\n","    'lr': 1e-4,           # Learning rate.\n","    'weight_decay': 5e-4, # L2 penalty.\n","    'momentum': 0.9,      # Momentum.\n","    'num_workers': 3,     # Number of workers on data loader.\n","    'batch_size': 8,      # Mini-batch size.\n","    'w_size': 224,        # Width size for image resizing.\n","    'h_size': 224,        # Height size for image resizing.\n","    'show_freq': 5,      # Show predictions in images each show_freq epochs.\n","}\n","\n","if torch.cuda.is_available():\n","    args['device'] = torch.device('cuda')\n","else:\n","    args['device'] = torch.device('cpu')\n","\n","print(args['device'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tN5ih4zfpqtd","colab_type":"text"},"source":["## Métricas usadas para Segmentação Semântica\n","\n","As métricas mais utilizadas na área de segmentação semântica (e em algumas aplicações em terefas de detecção também) são variações da Intersection over Union ($IoU$), também chamada de **Jaccard**.\n","\n","![IoU](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Intersection_over_Union_-_poor%2C_good_and_excellent_score.png/300px-Intersection_over_Union_-_poor%2C_good_and_excellent_score.png)\n","\n","Em tarefas de segmentação/detecção binária, se usa a $IoU$ simples, dada pela fórmula:\n","\n","$IoU = \\frac{\\|A\\ \\cap\\ B\\|}{\\|A\\ \\cup\\ B\\|} = \\frac{TP}{TP + FP + FN}$.\n","\n","No caso de problemas multiclasse, usa-se a média do $IoU$ para todas as classes, chamada de mean Intersection over Union ($mIoU$).\n","\n","A métrica F1 (conhecida também como **Dice**) também é utilizada nesses contextos e é dada pela fórmula:\n","\n","$F1 = \\frac{2TP}{2TP + FP + FN}$."]},{"cell_type":"code","metadata":{"id":"A83FXNdkprno","colab_type":"code","colab":{}},"source":["# Computing mean Intersection over Union (mIoU).\n","def evaluate(prds, labs):\n","    \n","    int_sum = np.zeros(args['n_classes'], dtype=np.float32)\n","    uni_sum = np.zeros(args['n_classes'], dtype=np.float32)\n","    \n","    for prd, lab in zip(prds, labs):\n","        \n","        for c in range(args['n_classes']):\n","\n","            union = np.sum(lab.ravel() == c)\n","            if union > 0:\n","                uni_sum[c] += union\n","                \n","                intersection = np.sum(np.logical_and(lab.ravel() == c, prd.ravel() == c))\n","                int_sum[c] += intersection\n","    \n","    return int_sum, uni_sum"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhJgCya1pu02","colab_type":"text"},"source":["# O dataset Pascal VOC 2012\n","\n","O dataset da competição [Pascal VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) contém rótulos para detecção e segmentação de imagens, como pode ser visto nas imagens abaixo. Ele e o dataset [COCO](http://cocodataset.org/) são os principais benchmarks da área de Visão Computacional atualmente em tarefas de segmentação semântica e detecção em imagens.\n","\n","![](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21.jpg)![](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21_class.png)"]},{"cell_type":"markdown","metadata":{"id":"kizq42nYpvz0","colab_type":"text"},"source":["O Pytorch recentemente lançou um dataloader específico pro Pascal VOC 2012."]},{"cell_type":"code","metadata":{"id":"q6YXnAS_pyrD","colab_type":"code","colab":{}},"source":["# Root directory for VOC.\n","root = './'\n","\n","# Classes left in Pascal VOC 2012.\n","voc_classes = [0, 89, 147]\n","\n","# All classes in Pascal VOC 2012.\n","# voc_classes = [0, 14, 19, 33, 37, 38, 52, 57, 72, 75, 89, 94, 108, 112, 113, 128, 132, 147, 150, 220]\n","\n","# Custom Image Transform.\n","class VOCImgTransform(object):\n","    \n","    def __call__(self, img):\n","#         img = transforms.functional.five_crop(img, (args['h_size'], args['w_size']))\n","        img = transforms.functional.center_crop(img, (args['h_size'], args['w_size']))\n","        img = transforms.functional.to_tensor(img)\n","        img = transforms.functional.normalize(img,\n","                                              mean=[0.485, 0.456, 0.406],\n","                                              std=[0.229, 0.224, 0.225])\n","        \n","#         print('img', img.size(), img.type())\n","        \n","        return img\n","\n","# Custom Target Transform.\n","class VOCTarTransform(object):\n","    \n","    def to_sequential_labels(self, tar):\n","        \n","        for i, c in enumerate(voc_classes):\n","            tar[tar == c] = i\n","        \n","        tar[tar > 2] = 0\n","        return tar\n","    \n","    def __call__(self, tar):\n","        tar = tar.convert(mode='I')\n","        tar = transforms.functional.center_crop(tar, (args['h_size'], args['w_size']))\n","        tar = transforms.functional.to_tensor(tar).squeeze().type(torch.int64)\n","        \n","        tar = self.to_sequential_labels(tar)\n","        \n","        return tar\n","\n","\n","# Setting datasets.\n","download_voc = not os.path.isdir('./VOCdevkit/')\n","train_set = datasets.VOCSegmentation(root,\n","                                     image_set='train',\n","                                     download=download_voc,\n","                                     transform=VOCImgTransform(),\n","                                     target_transform=VOCTarTransform())\n","val_set = datasets.VOCSegmentation(root,\n","                                   image_set='val',\n","                                   download=False,\n","                                   transform=VOCImgTransform(),\n","                                   target_transform=VOCTarTransform())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PQwXZl1Xp7RZ","colab_type":"text"},"source":["Exibindo algumas imagens do Pascal VOC 2012 e seus respectivos rótulos a nível de pixel."]},{"cell_type":"code","metadata":{"id":"P2TZp8pVp8C4","colab_type":"code","colab":{}},"source":["# Filtering VOC dataset to iterate only over samples from predefined classes.\n","train_indices = []\n","val_indices = []\n","\n","valid_classes = [0, 1, 2]\n","    \n","for i, batch_data in enumerate(train_set):\n","    \n","    imgs, labs = batch_data\n","    \n","    curr_labels = set(list(labs.numpy().ravel()))\n","    \n","    if len([c for c in valid_classes if c in curr_labels]) >= 2: \n","    \n","        train_indices.append(i)\n","        \n","        if i < 50:\n","            \n","            fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n","    \n","            ax[0].imshow(imgs.numpy().transpose(1, 2, 0))\n","            ax[0].set_yticks([])\n","            ax[0].set_xticks([])\n","            ax[0].set_title('Image')\n","\n","            ax[1].imshow(labs.numpy().squeeze() * 127, cmap=plt.get_cmap('gray'),\n","                         vmin=0, vmax=255)\n","            ax[1].set_yticks([])\n","            ax[1].set_xticks([])\n","            ax[1].set_title('Mask')\n","\n","            plt.show()\n","            \n","for i, batch_data in enumerate(val_set):\n","    \n","    imgs, labs = batch_data\n","    \n","    curr_labels = set(list(labs.numpy().ravel()))\n","    \n","    if len([c for c in valid_classes if c in curr_labels]) >= 2:\n","    \n","        val_indices.append(i)\n","\n","print(len(train_set), len(train_indices))\n","print(len(val_set), len(val_indices))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_BaqH0JqCP2","colab_type":"text"},"source":["Definindo dataloader customizado baseado no subsample do dataset padrão Pascal VOC 2012."]},{"cell_type":"code","metadata":{"id":"9kZRhkKwqBxa","colab_type":"code","colab":{}},"source":["# Sampler for limiting classes to humans and automobiles.\n","sampler_train = data.SubsetRandomSampler(train_indices)\n","sampler_val = data.SubsetRandomSampler(val_indices)\n","\n","# Setting dataloaders.\n","train_loader = DataLoader(train_set,\n","                          batch_size=args['batch_size'],\n","                          num_workers=args['num_workers'],\n","                          sampler=sampler_train)\n","val_loader = DataLoader(val_set,\n","                        batch_size=1,\n","                        num_workers=args['num_workers'],\n","                        sampler=sampler_val)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xksxToLRqKeH","colab_type":"text"},"source":["# Atividade Prática\n","\n","Implemente uma FCN com o backbone de uma VGG16 **pré-treinada** completando o código abaixo. \n","> Use a VGG16 com batch normalization: https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16_bn\n","\n","### Na função `__init__()`\n","\n","* O `backbone` deve incluir apenas as camadas de `features` da VGG16. Caso necessário, imprima a arquitetura da VGG16 para entender melhor essa instrução.\n","\n","* Acrescente uma sequência de camadas (`classifier`) para classificar as features extraídas pelo backbone.\n","**Sugestão:** 1 bloco convolucional composto por convolução, batch normalization, ativação ReLU e Dropout + 1 convolução final.\n","\n","### Na função `forward()`\n","\n","* Realize o forward no backbone \n","\n","* Realize um **upsample bilinear** para recuperar a resolução original da imagem.\n","\n","* Forward nas camadas de classificação."]},{"cell_type":"code","metadata":{"id":"t1wpc4ufqPGQ","colab_type":"code","colab":{}},"source":["class FCN_VGG16(nn.Module):\n","\n","    def __init__(self, num_classes, pretrained=True):\n","\n","        super(FCN_VGG16, self).__init__()\n","\n","        ## TODO: Definir Arquitetura\n","\n","    def forward(self, x):\n","\n","        ## TODO: forward\n","        \n","\n","## TODO: Definir a rede\n","print(net)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-ukE4wKvvqv","colab_type":"text"},"source":["## Optimizer\n","\n","**Vamos fazer um fine-tuning diferente!**\n","\n","Com o otimizador **Adam** do pacote `torch.optim` defina diferentes taxas de aprendizado para os parâmetros de sua rede. <br>\n","Documentação (Per-parameter options): https://pytorch.org/docs/stable/optim.html\n","\n","* Para o backbone (VGG16) defina $10\\%$ da taxa de aprendizado definida em `args['lr']`\n","* Para a sequência de classificação, defina `args['lr']` como taxa de aprendizado.\n","* Por fim, defina os últimos hiperparâmetros\n","```python\n","weight_decay=args['weight_decay'],\n","betas=(args['momentum'], 0.999)\n","```"]},{"cell_type":"code","metadata":{"id":"kzmKBQ7Qvv9Z","colab_type":"code","colab":{}},"source":["optimizer = ## TODO"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3LDLt3cXzKQP","colab_type":"text"},"source":["## Loss\n","\n","Como segmentação é uma tarefa de classificação de pixels, a loss deve ser novamente a Cross Entropy. A única diferença para a tarefa de classificação da aula passada é que as dimensões espaciais dos labels $(224, 224)$ e das predições da rede $(\\#classes, 224, 224)$ devem ser linearizadas para o cálculo da loss, resultado em vetores de dimensões $(224*224)$ e $(\\#classes, 224*224)$. Essa linearização deve ser implementada nas funções *train()* e *test()*."]},{"cell_type":"code","metadata":{"id":"UD-rXLyVwHBj","colab_type":"code","colab":{}},"source":["criterion = ## TODO"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ypvwoSjU0Gz5","colab_type":"text"},"source":["## Training & Validation"]},{"cell_type":"code","metadata":{"id":"fv3MbDpAwHsb","colab_type":"code","colab":{}},"source":["# Training function.\n","def train(train_loader, net, criterion, optimizer, epoch):\n","\n","    tic = time.time()\n","    \n","    # Setting network for training mode.\n","    net.train()\n","\n","    # Lists for losses and metrics.\n","    train_loss = []\n","    \n","    int_all = np.asarray(args['n_classes'], dtype=np.float32)\n","    uni_all = np.asarray(args['n_classes'], dtype=np.float32)\n","\n","    # Iterating over batches.\n","    for i, batch_data in enumerate(train_loader):\n","\n","        # Obtaining images and labels for batch.\n","        inps, labs = batch_data\n","\n","        # Casting to cuda variables.\n","        inps = inps.to(args['device'])\n","        labs = labs.to(args['device'])\n","\n","        # Clears the gradients of optimizer.\n","        optimizer.zero_grad()\n","\n","        # TO DO: Forwarding through network.\n","        outs = net(inps)\n","\n","        # TO DO: Computing loss.\n","        loss = criterion(outs.view(outs.size(0), outs.size(1), -1),\n","                         labs.view(labs.size(0), -1))\n","\n","        # Computing backpropagation.\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Obtaining predictions.\n","        prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n","        \n","        # Appending metrics for epoch error calculation.\n","        int_sum, uni_sum = evaluate([prds],\n","                                    [labs.detach().squeeze(0).cpu().numpy()])\n","\n","        int_all = int_all + int_sum\n","        uni_all = uni_all + uni_sum\n","\n","        # Updating loss meter.\n","        train_loss.append(loss.data.item())\n","\n","    toc = time.time()\n","    \n","    # Transforming list into numpy array.\n","    train_loss = np.asarray(train_loss)\n","    \n","    # Computing error metrics for whole epoch.\n","    iou = 0\n","    iou = np.divide(int_all, uni_all)\n","#     print(iou)\n","\n","    # Printing training epoch loss and metrics.\n","    print('-------------------------------------------------------------------')\n","    print('[epoch %d], [train loss %.4f +/- %.4f], [miou %.4f], [time %.4f]' % (\n","        epoch, train_loss.mean(), train_loss.std(), iou[1:].mean(), (toc - tic)))\n","    print('-------------------------------------------------------------------')\n","\n","def validate(val_loader, net, criterion, epoch):\n","\n","    tic = time.time()\n","    \n","    # Setting network for evaluation mode.\n","    net.eval()\n","\n","    # Lists for losses and metrics.\n","    val_loss = []\n","\n","    int_all = np.asarray(args['n_classes'], dtype=np.float32)\n","    uni_all = np.asarray(args['n_classes'], dtype=np.float32)\n","\n","    # Iterating over batches.\n","    for i, batch_data in enumerate(val_loader):\n","\n","        # Obtaining images and labels for batch.\n","        inps, labs = batch_data\n","\n","        # Casting to cuda variables.\n","        inps = inps.to(args['device'])\n","        labs = labs.to(args['device'])\n","\n","        # TO DO: Forwarding through network.\n","        outs = net(inps)\n","\n","        # TO DO: Computing loss.\n","        loss = criterion(outs.view(outs.size(0), outs.size(1), -1),\n","                         labs.view(labs.size(0), -1))\n","\n","        # Obtaining predictions.\n","        prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n","\n","        # Appending metrics for epoch error calculation.\n","        int_sum, uni_sum = evaluate([prds],\n","                                    [labs.detach().squeeze(0).cpu().numpy()])\n","\n","        int_all = int_all + int_sum\n","        uni_all = uni_all + uni_sum\n","\n","        # Updating loss meter.\n","        val_loss.append(loss.data.item())\n","        \n","        if i == 0 and epoch % args['show_freq'] == 0:\n","            \n","            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","            \n","            ax[0].imshow(inps[0].detach().cpu().numpy().transpose(1, 2, 0))\n","            ax[0].set_yticks([])\n","            ax[0].set_xticks([])\n","            ax[0].set_title('Image')\n","            \n","            ax[1].imshow(prds * 127,\n","                         cmap=plt.get_cmap('gray'),\n","                         vmin=0, vmax=255)\n","            ax[1].set_yticks([])\n","            ax[1].set_xticks([])\n","            ax[1].set_title('Prediction')\n","            \n","            ax[2].imshow(labs[0].detach().squeeze(0).cpu().numpy() * 127,\n","                         cmap=plt.get_cmap('gray'),\n","                         vmin=0, vmax=255)\n","            ax[2].set_yticks([])\n","            ax[2].set_xticks([])\n","            ax[2].set_title('True Label')\n","            \n","            plt.show()\n","\n","    toc = time.time()\n","    \n","    # Transforming list into numpy array.\n","    val_loss = np.asarray(val_loss)\n","    \n","    # Computing error metrics for whole epoch.\n","    iou = 0\n","    iou = np.divide(int_all, uni_all)\n","#     print(iou)\n","\n","    # Printing test epoch loss and metrics.\n","    print('-------------------------------------------------------------------')\n","    print('[epoch %d], [test loss %.4f +/- %.4f], [miou %.4f], [time %.4f]' % (\n","        epoch, val_loss.mean(), val_loss.std(), iou[1:].mean(), (toc - tic)))\n","    print('-------------------------------------------------------------------')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LN977gIFwRy7","colab_type":"code","colab":{}},"source":["# Iterating over epochs.\n","for epoch in range(1, args['epoch_num'] + 1):\n","\n","    # Training function.\n","    train(train_loader, net, criterion, optimizer, epoch)\n","\n","    # Computing test loss and metrics.\n","    validate(val_loader, net, criterion, epoch)"],"execution_count":0,"outputs":[]}]}